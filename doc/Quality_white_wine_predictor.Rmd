---
title: "Quality white wine predictor"
bibliography: reference.bib
output: 
  github_document:
    toc: True
  html_document: 
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(knitr)
```

# Summary

This report uses the white wine database from "vinho verde" to predict the quality based on physicochemical properties. Quality is a subjective measure, given by the average grade of three experts.

Before starting the predictions, the report makes a Explanatory data analysis (EDA) to look for features that may provide good prediction results, and also makes an short explanation about the metrics used in the models. In data preparation, the database are downloaded and processed in python. In this phase, the training and testing sets are created and they will be used during the model building.

There's a brief explanation of the models used in this report. Other important machine learning concepts, such as ensemble and cross validation, are also discussed.

The results section presents the best model for predicting quality and discuss why it was chosen for this purpose.

# Goal

This project aims to determine a model to predict wine quality given measurable wine features.

# Introduction

According to experts, wine is differentiated according to its smell, flavour, and colour, but most people are not wine experts to say that wine is good or bad. The quality of the wine is determined by many variables including, but not limited to, the ones mentioned previously. The quality of a wine is important for the consumers as well as the wine industry. For instance, industry players are using product quality certifications to promote their products. However, this is a time-consuming process and requires the assessment given by human experts, which makes this process very expensive. Nowadays, machine learning models are important tools to replace human tasks and, in this case, a good wine quality prediction can be very useful in the certification phase. For example, an automatic predictive system can be integrated into a decision support system, helping the speed and quality of the performance.

# Data

The wine quality dataset is publicly available on the UCI machine learning repository (check the kinks below). The dataset has two files, red wine and white wine variants of the Portuguese "Vinho Verde" wine. It contains a large collection of datasets that have been used for the machine learning community. The red wine dataset contains 1599 instances and the white wine dataset contains 4898 instances. Both files contain 11 input features and 1 output feature. Input features are based on the physicochemical tests and output variable based on sensory data is scaled in 11 quality classes from 0 to 10 (0-very bad to 10-very good)

-   [UCI repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)
-   [Wite wine database](http://open.canada.ca/data/en/dataset/b374f60b-9580-44dc-83f6-c0a850c15f30)

Input variables:

1.  **Alcohol**: the amount of alcohol in wine
2.  **Volatile acidity**: are high acetic acid in wine which leads to an unpleasant vinegar taste
3.  **Sulphates**: a wine additive that contributes to SO2 levels and acts as an antimicrobial and antioxidant
4.  **Citric Acid**: acts as a preservative to increase acidity (small quantities add freshness and flavor to wines)
5.  **Total Sulfur Dioxide**: is the amount of free + bound forms of SO2
6.  **Density**: sweeter wines have a higher density
7.  **Chlorides**: the amount of salt in the wine
8.  **Fixed acidity**: are non-volatile acids that do not evaporate readily
9.  **pH**: the level of acidity
10. **Free Sulfur Dioxide**: it prevents microbial growth and the oxidation of wine
11. **Residual sugar**: is the amount of sugar remaining after fermentation stops. The key is to have a perfect balance between â€” sweetness and sourness (wines > 45g/ltrs are sweet)

# Methods

**How to analyze the data** Our task here is to focus on what white wine features are important to get the promising result. For the purpose of classification model and evaluation of the relevant features, we are using the following algorithms to perform this task:

1.  **DummyRegressor()**: is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome.
2.  **Ridge**: is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values to be far away from the actual values.
3.  **Random Forest**: is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned
4.  **KNN**: is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in.The k-nearest-neighbor is an example of a "lazy learner" algorithm, meaning that it does not build a model using the training set until a query of the data set is performed.
5.  **Bayes**: is an algorithm that uses Bayes' theorem to classify objects. Naive Bayes classifiers assume strong, or naive, independence between attributes of data points. Popular uses of naive Bayes classifiers include spam filters, text analysis and medical diagnosis.
6.  **SVM**: is machine learning algorithm that analyzes data for classification and regression analysis. SVM is a supervised learning method that looks at data and sorts it into one of two categories. An SVM outputs a map of the sorted data with the margins between the two as far apart as possible.

# Analysis


## **Data Cleaning and Preprocessing**




## **EDA first conclusions**

According to our first EDA, we do not have a balanced database, our wines are concentrated around quality 5 and 7.5 (around 80% of data points). Besides, we have a couple of signs about some variables. For instance, it appears that the higher the alcohol level, the better the wine quality. Additionally, the smaller the chlorides and total sulphur dioxide the better the wine quality. Some variables seem do not influence wine quality on their own. When combining these variables, they might indeed influence wine quality.

According to our first EDA, we do not have a balanced database, our wines are concentrated around quality 5 and 7.5 (around 80% of data points). The image bellow can clear shows up this first finding:

```{r}
knitr::include_graphics("../results/Distribution_of_white_wine_quality.PNG")


```

Besides, we have a couple of signs about some variables. For instance, it appears that the higher the alcohol, Sulphates and ph levels the better the wine quality. We can check these information on the chart above:

```{r}
knitr::include_graphics("../results/relationship_between_individual_features_and_the_quality_3.PNG")


```


Following the same idea, we there are some evidences that the smaller the chlorides, Free Sulfur Dioxide, total sulphur dioxide and density levels, the better the wine quality. The chart bellow ilustrates these findings:

```{r}
knitr::include_graphics("../results/relationship_between_individual_features_and_the_quality_2.PNG")


```

For some variables like Fixed acidity, Volatile acidity, Citric Acid and Residual sugar seem do not influence wine quality on their own. However, we need to take care with this statement since when they are combining with other variables above, they might indeed influence wine quality. The chart bellow can show up these findings:

```{r}
knitr::include_graphics("../results/relationship_between_individual_features_and_the_quality_1.PNG")


```

## **Cross Validation**




# Results & Discussion

We gonna use the test score to decide what is the best model to our quality predictor xxxx

## References
